\section{\textit{Online Gradient Descent}}

Il \textit{Perceptron} accede al \textit{training set} in maniera sequenziale,
elaborando ogni esempio in tempo $\Theta(d)$ dove $d$ è il numero di 
\textit{feature}. Gli algoritmi che imparano sequenzialmente, come il
\textit{Perceptron}, sono anche ottimi nel gestire scenari dove nuovi dati di
\textit{training} vengono generati in ogni momento; alcuni esempi sono dati
generati da sensori, mercati finanziari o interazioni. In questi casi, i
protocolli tradizionali di apprendimento basati sull'utilizzo di
\textit{training set} di dimensione fissata, risultano inefficienti, siccome
si dovrebbe far girare da capo il \textit{learning algorithm} ogni volta che
si aggiungono nuovi esempi nel \textit{training set}.

\subsection{\textit{Online learning}}
Una generalizzione di questo tipo di \textit{learning algorithm} sequenziale,
che si dirà \textit{online learning}, può essere riassunta come segue.
\begin{algorithm}
    \DontPrintSemicolon
    \Params{Classe $\Hpred$ di predittori, funzione di \textit{loss} $\ell$}\;
    L'algoritmo genera un predittore iniziale di default $h_1\in\Hpred$\;
    \For{$t=1,2,\dots$}{
        1. Il prossimo esempio $(x_t,y_t)$ viene osservato\;
        2. L'errore $\ell(h_t(x_t),y_t)$ del predittore corrente $h_t$
            viene calcolato\;
        3. $h_t$ viene aggiornato generando un nuovo predittore 
            $h_{t+1}\in\Hpred$}
\end{algorithm}
\vspace{-1.5em}

Una caratteristica tipica dell'\textit{online learning} è che l'aggiornamento
del modello $h_{t+1}\in\Hpred$ è locale, ovvero dipende solo dal predittore
attuale e dall'esempio attuale.

\subsection{Analisi dell'errore di \textit{Perceptron}}
Un esempio di algoritmo di \textit{online learning} è il \textit{Perceptron}.
Si analizzerà ora il numero massimo di errori che \textit{Perceptron} fa su
un qualsiasi flusso di esempi, anche se linearmente non separabili.

Sia $M$ il numero di errori commessi dal \textit{Perceptron}; si ha che:
$$\forall u\in\RN^d \qquad
M \leq \sum_{t=1}^{T}h_t(u)+(\|u\|X)^2+\|u\|X\sqrt{\sum_{t=1}^{T}h_t(u)} $$
dove:
\begin{itemize}
    \item $h_t(u)$ è la \textbf{\textit{hinge loss}} al tempo $t$ del vettore
        $u$: $h_t(u) = \max{\{0,1-y_tu^\top x_t\}}$
    \item $X$ è la norma massima dei \textit{data point}: $X=\max_t{\|x_t\|}$
\end{itemize}

\subsection{Rischio sequenziale e \textit{regret}}
Un predittore algoritmo \textit{online} $A$ genera una sequenza
$h_1,h_2,\dots,\in\Hpred$ di predittori. Per valutarne le performance si userà
il \textbf{rischio sequenziale}
$$ \frac{1}{T}\sum_{t=1}^{T}\ell(h_t(x_t),y_t) $$
che misura la \textit{loss} media della sequenza di predittori sui primi $T$
esempi.

Un altra misura di valutazione è la \textbf{\textit{regret}}
$$
\frac{1}{T}\sum_{t=1}^{T}\ell_t(h_t)-
\min_{h\in\Hpred}\frac{1}{T}\sum_{t=1}^{T}\ell_t(h)
$$
che misura la differenza tra il rischio sequenziale dei primi $T$ predittori
e il rischio sequenziale del miglior predittore presente in $\Hpred$. La
\textit{regret} può essere visto come la controparte sequenziale dell'errore
di variazione dello \textit{statistical learning}.

\subsection{\textit{Projected Gradient Descent}}
Si vedrà ora la versione \textit{online} del \textit{gradient descent},
algoritmo predominante per l'ottimizzazione di funzioni convesse e derivabili.
Data una funzione $f$ convessa e differenziabilie, di cui si vuole trovare il 
minimo, il \textit{gradient descent} funziona iterando 
$w_{t+1}=w_t-\eta_t\nabla f(w_t)$ partendo da un punto iniziale $w_1$ presente
nel dominio di $f$, dove $\eta>0$ è un parametro. Preso il punto $w_t$, se
$\nabla f(w_t)\neq 0$, allora $w_t$ non è un minimo e l'algoritmo prenderà
un nuovo punto $w_{t+1}$ andando nella direzione opposta alla direzione del
gradiente $\nabla f(w_t)$.

Per poter analizzare l'\textit{Online Gradient Descent} (OGD), bisogna
comprendere il comportamento del \textit{gradient descent} quando la funzione
$f$ da minimizzare cambia ad ogni iterazione. Ci si concentrerà su predittori
lineari $h(x)=w^\top x$ con $w\in\RN^d$. Si userà la notazione
$\ell_t(w) = \ell(w^\top x_t,y_t)$ e si assumerà che le \textit{loss}
$\ell_1,\ell_2,\dots$ siano convesse e derivabili in tutti i loro punti.

Viene ora mostrato l'algoritmo dell'\textit{projected OGD}:

\begin{algorithm}[H]
    \DontPrintSemicolon
    \Params{$\eta>0,\ U>0$}\;
    $w_1 = 0\qquad$ \hspace{9.6em}\tcp*[h]{Inizializzazione}\;
    \For{$t=1,2,\dots$}{
        $w_{t+1}'=w_t-\displaystyle\frac{\eta}{\sqrt{t}}\nabla\ell_t(w_t)$\;
        $w_{t+1}=\displaystyle\argmin_{w:\|w\|\leq U}{\|w-w_{t+1}'\|}$
        \hspace{1em}\tcp*[h]{Proiezione}
    }
    \caption{\textit{Projected OGD}}
\end{algorithm}
Il parametro $U$ indica il raggio di una sfera dentro la quale cercare il
punto con la minor \textit{regret} (proiezione).

\subsubsection{Analisi \textit{regret}}
Sia $G$ il valore massimo che la norma del gradiente può assumere:
$$ \forall t\leq T \quad \| \nabla\ell_t(w_t)\|\leq G  $$
Si può mostrare come, con qualsiasi funzione di \textit{loss} si ha:
$$
\underbrace{\frac{1}{T}\sum_{t=1}^{T}\ell_t(w_t)}_{\displaystyle\ell_T(w)}\leq
\underbrace{\min_{u:\|u\|\leq U} \frac{1}{T}\sum_{t=1}^T \ell_t(u)}_
    {\displaystyle\ell_T(u^*)}
+\underbrace{UG\sqrt{\frac{8}{T}}}_{\displaystyle\text{\textit{regret}}}
$$
dove:
\begin{itemize}
    \item $\ell_T(w)$ è la \textit{loss} media dell'algoritmo fino al passo $T$;
    \item $\ell_t(u^*)$ è la \textit{loss} media del miglior predittore trovato
        fino al passo $T$;
    \item $UG\sqrt{\frac{8}{T}}$ è la \textit{regret} dell'algoritmo.
\end{itemize}

\subsection{OGD con \textit{loss} fortemente convesse}
Il limite mostrato nella precedente sezione non può essere migliorato in presenza
di funzioni di \textit{loss} lineari. Si può invece migliore in presenza di
funzioni di \textit{loss} convesse e mai piatte. Per formalizzare questo scenario
verrà introdotto il concetto di funzione fortemente convessa.

Una funzione differenziabilie $\ell$ è $\sigma$-fortemente convessa con $\sigma>0$
se, per ogni $w,u \in RN^d$ vale che:
$$ \ell(w) - \ell(u) \leq \nabla\ell(w)^\top (w-u)-\frac{\sigma}{2}\|u-w\|^2 $$
o, in altri termini, è una funzione che cresce alla stessa velocità di una funzione
quadratica.

L'algoritmo di OGD per funzioni fortemente convesse non necessita del passo di
proiezione.

\begin{algorithm}[H]
    \DontPrintSemicolon
    $w_1 = 0\qquad$ \;
    \For{$t=1,2,\dots$}{
        $w_{t+1}'=w_t-\displaystyle\frac{1}{\sigma t}\nabla\ell_t(w_t)$\;
    }
    \caption{\textit{OGD} per funzioni fortemente convesse}
\end{algorithm}

\subsubsection{Analisi \textit{regret}}
L'analisi della \textit{regret} mostra che:
$$
\underbrace{\frac{1}{T}\sum_{t=1}^{T}\ell_t(w_t)}_{\displaystyle\ell_T(w)}\leq
\underbrace{\min_{u\in\RN^d} \frac{1}{T}\sum_{t=1}^T \ell_t(u)}_
    {\displaystyle\ell_T(u^*)}
+\underbrace{\frac{G^2(1+\ln{T})}{2\sigma T}}_{\displaystyle\text{\textit{regret}}}
$$
dove:
\begin{itemize}
    \item $\sigma$ indica che la \textit{loss} è $\sigma$-fortemente convessa;
    \item $G$ è il valore massimo che la norma del gradiente può assumere: 
        $\max_t{\|\nabla\ell_t(w_t)\|\leq G}$;
\end{itemize}