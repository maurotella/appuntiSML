\section{Gli algoritmi \textit{Nearest Neighbor}}

\subsection{\textit{Nearest Neighbor} (NN)}

\subsubsection{Definizione}
Verrà introdotto ora l'algoritmo di \textbf{\textit{Nearest Neighbor} (NN)} per la
classificazione binaria con \textit{feature} numeriche:
$$ \X = \RN^d \qquad\qquad \Y = \{-1,1\} $$

NN non è un'istanza di ERM in quanto non punta a minimizzare $\loss_S$.

\textbf{L'idea di NN è la sueguente:
\begin{itemize}
    \item Predici ogni punto del \textit{training set} con la propria etichetta;
    \item Predici gli altri punti con l'etichetta del punto del \textit{training set}
        che è più vicino al punto interessato.
\end{itemize}
}

Più formalmente, dato un \textit{training set}:
$$ S = \{(x_1,y_1),\dots,(x_m,y_m)\} $$
l'algoritmo $\Ann$ genera un classificatore $\hnn:\RN \rightarrow \{-1,1\}$ definito
come segue:
$$ \hnn(x) = \text{etichetta $y_t$ del punto $x_t \in S$ più vicino a x} $$

Se a minimizzare la distanza con $x$ sono più punti, si predirrà l'etichetta più
presente tra i putni vicini. Se non c'è una maggioranza di etichette tra i punti
più vicini si predirrà un valore di default $\in \{-1,1\}$.

Presi due punti $x=(x_1,\dots,x_d)$ e $x_t=(x_{t,1},\dots,x_{t,d})$, la distanza
$||x-x_t||$ verrà calcolata tramite la distanza euclidea:
$$||x-x_t|| = \sqrt{\sum_{i=1}^d (x_i-x_{t,i})^2}$$

Ogni classificatore binario $\pred : \RN^d \rightarrow \{-1,1\}$ partiziona $\RN^d$
in due regioni (come mostrato in figura \ref{fig:voronoi}):
$$ \colorbox{transp-cyan}{$\{ x \in \RN^d : \pred(x)=1 \}$} \quad , \quad 
   \colorbox{transp-orange}{$\{ x \in \RN^d : \pred(x)=-1 \}$} $$

\begin{figure}[h]
    \centering
    \input{figures/voronoi.tex}
    \caption{Diagramma di Voronoi in $\RN^2$\label{fig:voronoi}; tutti i punti $x$
    interni a una cella con centro \textbullet $x_t$ sono tali che $\hnn(x)=y_t$}
\end{figure}

\subsubsection{Efficienza ed efficacia}

Siccome il funzionamento di NN implica la memorizzazione di tutto il 
\textit{training set}, \textbf{l'algoritmo non scala bene con il numero di
$|S| = m$ di \textit{training point}}. Inoltre, calcolare un qualsiasi
$\hnn(x)$ è costoso, in quanto richiede di calcolare la distanza tra $x$ e
tutti gli altri punti di $S$; questo in $\RN^d$ comporta un costo di 
$\Theta (dm)$.

Infine, si noti come, vista la completa memorizzazione di $S$, \textbf{NN generi 
sempre un classificatore $\hnn$ con \textit{training error} nullo}:
$$ \loss_S(\hnn)=0 $$

\subsection{\texorpdfstring{$k-$Nearest Neighbor (\kNN)}{k-NN}}

\subsubsection{Definizione}
Partendo dagli algoritmi NN, si può ottenere una famiglia di algoritmi detta
\kNN; il parametro $k$ assume tipicamente i valori $k=1,3,5,\dots$ con $k<|S|$.

Questi algoritmi sono definiti come segue: dato un \textit{training set} $S$ e un
punto $x \in \X$, \kNN \ genererà un predittore $\hknn$ tale che:
$$\hknn(x) = \text{etichetta $y_t$ appartenente alla maggioranza dei $k$ punti
più vicini a $x$} $$

\begin{figure}[h]
    \centering
    \input{figures/knn.tex}
    \caption{Esempi di $\hknn$ con $\X=\RN^2$; si noti come, con lo stesso 
    \textit{training set}, la predizione cambia al variare di $k$. \label{fig:knn}}
\end{figure}
\vspace{1em}

\subsubsection{Efficienza ed efficacia}

A livello di efficienza \kNN \ soffre degli stessi problemi di NN vista la
memorizzazione dell'intero \textit{training set}.

Per quanto riguarda la sua efficacia invece, \kNN \ non ha sempre un 
\textit{training error} nullo:
$$ \loss_S(\hknn)\geq0 $$

\begin{figure}[h]
    \centering
    \input{figures/knn_line.tex}
    \caption{\label{fig:knn_line}Esempi di $\hknn$ con $\X = \RN$.}
\end{figure}
\vspace{1em}

Come si può infatti notare dalla figura \ref{fig:knn_line}, nei casi con $k=3$ e 
$k=5$ sono presenti punti errati (evidenziati in grassetto) considerati dal 
classificatore come \textit{outlier}. Inoltre \textbf{al crescere di $k$ cresce 
anche la \quotes{semplicità} del classificatore così come il numero di punti errati}.
L'estremo di ciò è quando $k=|S|$; in questo caso infatti $\hknn$ diventa un 
classificatore costante che predice sempre l'etichetta più presente in tutto $S$.

In un generico classificatore $\hknn$ tipicamente succede che:
\begin{itemize}
    \item \textbf{Se $k$ è troppo basso} si ottiene un classificatore che si 
        \quotes{fida} troppo del \textit{training set}, ottenendo quindi 
        \textbf{\textit{overfitting}};
    \item \textbf{Se $k$ è troppo alto}, si ottiene un classificatore troppo
        semplice, ottenendo \textbf{\textit{underfitting}}.
\end{itemize}

Tutti i classificatori introdotti fino ad'ora sono classificatori binari ($|\Y|=2$).
Tuttavia \kNN può essere usato anche per:
\begin{itemize}
    \item problemi di classificazione multiclasse ($|\Y|>2$): si opera come nel caso
        binario, predicendo quindi l'etichetta più presente nei $k$ punti più vicini;
    \item problemi di regressione ($\Y = \RN$): si predice la media aritmetice delle
        etichette dei $k$ punti più vicini.
\end{itemize}