\babel@toc {italian}{}\relax 
\babel@toc {italian}{}\relax 
\contentsline {section}{\numberline {1}Introduzione}{3}{section.1}%
\contentsline {subsection}{\numberline {1.1}Definizioni fondamentali}{3}{subsection.1.1}%
\contentsline {subsubsection}{\numberline {1.1.1}\textit {Label set} $\ensuremath {\mathcal {Y}}$}{3}{subsubsection.1.1.1}%
\contentsline {subsubsection}{\numberline {1.1.2}\textit {Loss function} $\ensuremath {\ell }$}{3}{subsubsection.1.1.2}%
\contentsline {subsubsection}{\numberline {1.1.3}\textit {Data domain} $\ensuremath {\mathcal {X}}$}{4}{subsubsection.1.1.3}%
\contentsline {subsubsection}{\numberline {1.1.4}Predittori $f$}{5}{subsubsection.1.1.4}%
\contentsline {subsubsection}{\numberline {1.1.5}Esempi}{5}{subsubsection.1.1.5}%
\contentsline {subsubsection}{\numberline {1.1.6}\textit {Test set} e \textit {test error}}{5}{subsubsection.1.1.6}%
\contentsline {subsubsection}{\numberline {1.1.7}\textit {Learning algorithm} $A$}{5}{subsubsection.1.1.7}%
\contentsline {subsubsection}{\numberline {1.1.8}\textit {Training error} $\ensuremath {\ell }_S$}{5}{subsubsection.1.1.8}%
\contentsline {subsection}{\numberline {1.2}Empirical Risk Minimization (ERM)}{6}{subsection.1.2}%
\contentsline {subsubsection}{\numberline {1.2.1}Definizione}{6}{subsubsection.1.2.1}%
\contentsline {subsubsection}{\numberline {1.2.2}Predittori con \textit {test error} elevato}{6}{subsubsection.1.2.2}%
\contentsline {subsubsection}{\numberline {1.2.3}\textit {Overfitting} e \textit {underfitting}}{7}{subsubsection.1.2.3}%
\contentsline {subsubsection}{\numberline {1.2.4}Etichette rumorose}{7}{subsubsection.1.2.4}%
\contentsline {section}{\numberline {2}Gli algoritmi \textit {Nearest Neighbor}}{8}{section.2}%
\contentsline {subsection}{\numberline {2.1}\textit {Nearest Neighbor} (NN)}{8}{subsection.2.1}%
\contentsline {subsubsection}{\numberline {2.1.1}Definizione}{8}{subsubsection.2.1.1}%
\contentsline {subsubsection}{\numberline {2.1.2}Efficienza ed efficacia}{9}{subsubsection.2.1.2}%
\contentsline {subsection}{\numberline {2.2}$k-$Nearest Neighbor ($k-$NN)}{9}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1}Definizione}{9}{subsubsection.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.2}Efficienza ed efficacia}{9}{subsubsection.2.2.2}%
\contentsline {section}{\numberline {3}\textit {Tree Predictors}}{11}{section.3}%
\contentsline {subsection}{\numberline {3.1}Definizione}{11}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}Costruzione di un \textit {tree predictor}}{12}{subsection.3.2}%
\contentsline {subsubsection}{\numberline {3.2.1}Idea generale}{12}{subsubsection.3.2.1}%
\contentsline {subsubsection}{\numberline {3.2.2}\textit {Training error}}{12}{subsubsection.3.2.2}%
\contentsline {subsubsection}{\numberline {3.2.3}Crescita dell'albero e \textit {training error}}{14}{subsubsection.3.2.3}%
\contentsline {subsubsection}{\numberline {3.2.4}Algoritmo generale}{15}{subsubsection.3.2.4}%
\contentsline {subsection}{\numberline {3.3}\textit {Overfitting}}{15}{subsection.3.3}%
\contentsline {subsection}{\numberline {3.4}Interpretabilità}{16}{subsection.3.4}%
\contentsline {section}{\numberline {4}\textit {Statistical Learning}}{17}{section.4}%
\contentsline {subsection}{\numberline {4.1}Definizioni}{17}{subsection.4.1}%
\contentsline {subsubsection}{\numberline {4.1.1}Rischio statistico $\ell _{\ensuremath {\mathcal {D}}}$}{17}{subsubsection.4.1.1}%
\contentsline {subsubsection}{\numberline {4.1.2}Predittore ottimo di Bayes $f^*$}{17}{subsubsection.4.1.2}%
\contentsline {subsubsection}{\numberline {4.1.3}Rischio condizionato}{17}{subsubsection.4.1.3}%
\contentsline {subsubsection}{\numberline {4.1.4}Rischio di Bayes $\ell _{\ensuremath {\mathcal {D}}}(f^*)$}{17}{subsubsection.4.1.4}%
\contentsline {subsection}{\numberline {4.2} $f^*$ e $\ell _\ensuremath {\mathcal {D}}$ nelle varie \textit {loss function} }{17}{subsection.4.2}%
\contentsline {subsubsection}{\numberline {4.2.1}\textit {Quadratic loss}}{18}{subsubsection.4.2.1}%
\contentsline {subsubsection}{\numberline {4.2.2}\textit {Zero-one loss}}{19}{subsubsection.4.2.2}%
\contentsline {subsection}{\numberline {4.3}Limitare il rischio}{19}{subsection.4.3}%
\contentsline {subsubsection}{\numberline {4.3.1}Stima del rischio}{19}{subsubsection.4.3.1}%
\contentsline {subsubsection}{\numberline {4.3.2}Chernoff-Hoeffding}{20}{subsubsection.4.3.2}%
\contentsline {subsection}{\numberline {4.4}Decomposizione \textit {bias-variance}}{21}{subsection.4.4}%
\contentsline {subsection}{\numberline {4.5}\textit {Overfitting} e \textit {underfitting}}{21}{subsection.4.5}%
\contentsline {section}{\numberline {5}Analisi del rischio sui \textit {tree predictor}}{25}{section.5}%
\contentsline {subsection}{\numberline {5.1}Limitare il numero di nodi}{25}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Limite superiore più dettagliato}{26}{subsection.5.2}%
\contentsline {subsubsection}{\numberline {5.2.1}\textit {Structural Risk Minimization} (SRM)}{26}{subsubsection.5.2.1}%
\contentsline {subsubsection}{\numberline {5.2.2}Funzione peso $w$}{26}{subsubsection.5.2.2}%
\contentsline {section}{\numberline {6}Iperparametri e stima del rischio}{28}{section.6}%
\contentsline {subsection}{\numberline {6.1}Valutazione tramite \textit {cross-validation} esterna}{28}{subsection.6.1}%
\contentsline {subsection}{\numberline {6.2}Modifica degli iperparametri}{28}{subsection.6.2}%
\contentsline {subsection}{\numberline {6.3}Modifica dei parametri tramite \textit {nested cross-validation}}{29}{subsection.6.3}%
\contentsline {section}{\numberline {7}Consistenza e algoritmi non parametrici}{30}{section.7}%
\contentsline {subsection}{\numberline {7.1}Consistenza}{30}{subsection.7.1}%
\contentsline {subsection}{\numberline {7.2}Algoritmi non parametrici}{30}{subsection.7.2}%
\contentsline {subsubsection}{\numberline {7.2.1}$k-$NN}{30}{subsubsection.7.2.1}%
\contentsline {subsubsection}{\numberline {7.2.2}Algoritmo greedy per \textit {tree classifier}}{30}{subsubsection.7.2.2}%
\contentsline {subsection}{\numberline {7.3}Algoritmi non consistenti e non parametrici}{31}{subsection.7.3}%
\contentsline {subsubsection}{\numberline {7.3.1}Teorema \textit {No Free Lunch}}{31}{subsubsection.7.3.1}%
\contentsline {subsection}{\numberline {7.4}Maledizione della dimensionalità}{31}{subsection.7.4}%
\contentsline {section}{\numberline {8}Analisi del rischio per gli algoritmi NN}{32}{section.8}%
